# Knowledge Distillation 和 Adversarial Training 与 NLP 相关的论文收集
分享一下自己看论文收集总结的一些paper. 持续更新~   欢迎大家来一起更新呀

### Adversarial Training
-   [Technical report on Conversational Question Answering](https://arxiv.org/pdf/1909.10772) 2019 问答任务中基于RoBERTa融合对抗样本以及知识蒸馏技术
-   [Generating Natural Language Adversarial Examples](https://arxiv.org/pdf/1804.07998.pdf)  2018 
-    [Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence with Adversarial Examples  ](https://arxiv.org/pdf/1803.01128.pdf)  2018
-    [ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION](https://arxiv.org/pdf/1605.07725.pdf) 2016 [code1](https://github.com/tensorflow/models/tree/master/research/adversarial_text)  [code2](https://github.com/TobiasLee/Text-Classification/blob/master/models/adversarial_abblstm.py)
-    [FREELB: ENHANCED ADVERSARIAL TRAINING FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1909.11764.pdf) 2019




### Knowledge Distillation（知识蒸馏）
- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf) 2019
- [TINYBERT: DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1909.10351.pdf) 2019
- [Technical report on Conversational Question Answering](https://arxiv.org/pdf/1909.10772) 2019 
- [Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/pdf/1903.12136.pdf) 2019
- [Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/pdf/1904.09482v1.pdf) 2019   [code](https://github.com/namisan/mt-dnn)



#### 其他资源

- [awesome-knowledge-distillation](https://github.com/dkozlov/awesome-knowledge-distillation )
- [Adversarial_Learning_Paper](https://github.com/Guo-Yunzhe/Adversarial_Learning_Paper)
- [Adversarial-Machine-Learning](https://github.com/tanjuntao/Adversarial-Machine-Learning)

- [PLMpapers](https://github.com/thunlp/PLMpapers)

